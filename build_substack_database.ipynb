{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Substack Text Database\n",
    "\n",
    "The goal of this is to build a database that we can use to identify interesting keywords, topics, writing patterns, and themes, etc. that are associated with higher levels of subscribers and engagement for writers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries, Setting Up Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "import sys\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = \"substack_database.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the blog_metadata table\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS blog_metadata (\n",
    "        blog_url TEXT PRIMARY KEY,\n",
    "        subscriber_count INTEGER,\n",
    "        public_post_count INTEGER,\n",
    "        private_post_count INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS article_data (\n",
    "        title TEXT,\n",
    "        audience TEXT,\n",
    "        canonical_url TEXT PRIMARY KEY,\n",
    "        description TEXT,\n",
    "        truncated_body_text TEXT,\n",
    "        wordcount INTEGER,\n",
    "        reaction_count INTEGER,\n",
    "        comment_count INTEGER,\n",
    "        post_date TEXT, \n",
    "        polarity REAL,\n",
    "        objectivity REAL,\n",
    "        number_of_questions INTEGER,\n",
    "        fk_grade_level REAL,\n",
    "        gunning_fog_index REAL,\n",
    "        reading_time REAL,\n",
    "        p_elem_counts INTEGER,\n",
    "        a_elem_counts INTEGER,\n",
    "        img_elem_counts INTEGER,\n",
    "        ul_elem_counts INTEGER,\n",
    "        li_elem_counts INTEGER,\n",
    "        video_elem_counts INTEGER,\n",
    "        br_elem_counts INTEGER,\n",
    "        tokens TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "\n",
    "# Commit changes and close the database connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Substack Blogs For Scraping\n",
    "Here we identify substack blogs and get the number of subscribers per blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the query and the desired website domain to search\n",
    "# it would be nice to have a better way to do this.... \n",
    "query = \"site:substack.com\"\n",
    "num_results = 1000\n",
    "\n",
    "# Create a list to store the URLs\n",
    "results = []\n",
    "\n",
    "# Perform the Google search and scrape the URLs\n",
    "for url in tqdm(search(query, num_results=num_results)):\n",
    "    results.append(url)\n",
    "    #time.sleep(1) # space out requests so we dont get blocked by google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://substack.com/',\n",
       " 'https://danperry.substack.com/',\n",
       " 'https://dellavolpe.substack.com/',\n",
       " 'https://managingeditor.substack.com/',\n",
       " 'https://susanality.substack.com/',\n",
       " 'https://afterschool.substack.com/',\n",
       " 'https://loleen.substack.com/',\n",
       " 'https://jeffreycarr.substack.com/',\n",
       " 'https://kjramseywrites.substack.com/',\n",
       " 'https://ismatu.substack.com/',\n",
       " 'https://theupheaval.substack.com/',\n",
       " 'https://peterbeinart.substack.com/',\n",
       " 'https://chamath.substack.com/',\n",
       " 'https://johannadrucker.substack.com/',\n",
       " 'https://mattstoller.substack.com/',\n",
       " 'https://petition.substack.com/',\n",
       " 'https://podcastthenewsletter.substack.com/',\n",
       " 'https://lg.substack.com/',\n",
       " 'https://nathanbenaich.substack.com/',\n",
       " 'https://adamset.substack.com/',\n",
       " 'https://caughtoffside.substack.com/',\n",
       " 'https://ryanmcbeth.substack.com/',\n",
       " 'https://codeconfessions.substack.com/',\n",
       " 'https://myclimatejourney.substack.com/',\n",
       " 'https://basu.substack.com/',\n",
       " 'https://whyisthisinteresting.substack.com/',\n",
       " 'https://thehockeywriters.substack.com/',\n",
       " 'https://andjelicaaa.substack.com/',\n",
       " 'https://theupandup.substack.com/',\n",
       " 'https://radleybalko.substack.com/',\n",
       " 'https://lordfed.substack.com/',\n",
       " 'https://themargins.substack.com/',\n",
       " 'https://alicebell.substack.com/',\n",
       " 'https://insidethenewsroom.substack.com/',\n",
       " 'https://bizzarodevs.substack.com/',\n",
       " 'https://substack.com/switch',\n",
       " 'https://newsnotnoisejessicayellin.substack.com/',\n",
       " 'https://reviewcanada.substack.com/',\n",
       " 'https://thecatholicfeminist.substack.com/',\n",
       " 'https://indignity.substack.com/',\n",
       " 'https://read.substack.com/',\n",
       " 'https://femchaospod.substack.com/',\n",
       " 'https://nancyj.substack.com/',\n",
       " 'http://harkaway.substack.com/',\n",
       " 'https://rwmalonemd.substack.com/',\n",
       " 'https://edwardsnowden.substack.com/',\n",
       " 'https://thecontender.substack.com/',\n",
       " 'https://mollyknight.substack.com/',\n",
       " 'https://stevevladeck.substack.com/',\n",
       " 'https://artificialintelligencemadesimple.substack.com/',\n",
       " 'https://boriquagato.substack.com/',\n",
       " 'https://anneboyer.substack.com/',\n",
       " 'https://substack.com/about',\n",
       " 'https://aseanwonk.substack.com/',\n",
       " 'https://techworldwithmilan.substack.com/',\n",
       " 'https://gptzero.substack.com/',\n",
       " 'https://wecanfixit.substack.com/',\n",
       " 'https://sheriffs.substack.com/',\n",
       " 'https://garyrosenblatt.substack.com/',\n",
       " 'https://interconnect.substack.com/',\n",
       " 'https://wordloaf.substack.com/',\n",
       " 'https://coloradomedia.substack.com/',\n",
       " 'https://artificialcorner.substack.com/',\n",
       " 'https://katz.substack.com/',\n",
       " 'https://equalityalec.substack.com/',\n",
       " 'https://wondertools.substack.com/',\n",
       " 'https://edwest.substack.com/',\n",
       " 'https://hotglobe.substack.com/',\n",
       " 'https://theuconnfastbreak.substack.com/',\n",
       " 'https://lauradodsworth.substack.com/',\n",
       " 'https://emilymcdowell.substack.com/',\n",
       " 'https://joshbrake.substack.com/',\n",
       " 'https://fasterplease.substack.com/',\n",
       " 'https://thewritersalmanac.substack.com/',\n",
       " 'https://jimmydoom.substack.com/',\n",
       " 'https://pickandrollau.substack.com/',\n",
       " 'https://jonn.substack.com/',\n",
       " 'https://matthewgreenglobal.substack.com/',\n",
       " 'https://reddmonitor.substack.com/',\n",
       " 'https://thepillar.substack.com/',\n",
       " 'https://endtimes.substack.com/',\n",
       " 'https://dusttodigital.substack.com/',\n",
       " 'https://jakobnielsenphd.substack.com/',\n",
       " 'https://steady.substack.com/',\n",
       " 'https://kareem.substack.com/',\n",
       " 'https://christianlorentzen.substack.com/',\n",
       " 'https://rychappell.substack.com/',\n",
       " 'https://paperalfa.substack.com/',\n",
       " 'https://tatlondon.substack.com/',\n",
       " 'https://annehelen.substack.com/',\n",
       " 'https://heathercoxrichardson.substack.com/',\n",
       " 'https://kcsn.substack.com/',\n",
       " 'https://theshiftwithsambaker.substack.com/',\n",
       " 'https://natesilver.substack.com/',\n",
       " 'https://gentlefoods.substack.com/',\n",
       " 'https://robertpjones.substack.com/',\n",
       " 'https://stainedpagenews.substack.com/',\n",
       " 'https://gingerriver.substack.com/',\n",
       " 'https://marissarothkopf.substack.com/',\n",
       " 'https://fakephdinvestigator.substack.com/',\n",
       " 'https://maggiesmith.substack.com/',\n",
       " 'https://eyelashroaming.substack.com/',\n",
       " 'https://truehoop.substack.com/',\n",
       " 'https://johncanzano.substack.com/',\n",
       " 'https://dinneralovestory.substack.com/',\n",
       " 'https://andrewlawton.substack.com/',\n",
       " 'https://bobsturm.substack.com/',\n",
       " 'https://stateofai.substack.com/',\n",
       " 'https://traditionsanity.substack.com/',\n",
       " 'https://twitterfiles.substack.com/',\n",
       " 'https://lauriestone.substack.com/',\n",
       " 'https://notdrinkingpoison.substack.com/',\n",
       " 'https://thehyphen.substack.com/',\n",
       " 'https://fallows.substack.com/',\n",
       " 'https://arimelber.substack.com/',\n",
       " 'https://figsinwinter.substack.com/',\n",
       " 'https://eamonnbrennan.substack.com/',\n",
       " 'https://quoththeraven.substack.com/',\n",
       " 'https://statuskuo.substack.com/',\n",
       " 'https://substack.com/events',\n",
       " 'https://zeynep.substack.com/',\n",
       " 'https://karenswallowprior.substack.com/',\n",
       " 'https://eugenesrobinson.substack.com/',\n",
       " 'https://fathernathan.substack.com/',\n",
       " 'https://jessica.substack.com/',\n",
       " 'https://claireberlinski.substack.com/',\n",
       " 'https://katecasey.substack.com/',\n",
       " 'https://tomkinstimes.substack.com/',\n",
       " 'https://importai.substack.com/',\n",
       " 'https://exxonknews.substack.com/',\n",
       " 'https://leefang.substack.com/',\n",
       " 'https://theclimatebrink.substack.com/',\n",
       " 'https://shannanmartin.substack.com/',\n",
       " 'https://seymourhersh.substack.com/',\n",
       " 'https://annebyrn.substack.com/',\n",
       " 'https://chrislatray.substack.com/',\n",
       " 'https://tamaradler.substack.com/',\n",
       " 'https://eatgordaeat.substack.com/',\n",
       " 'https://helenlewis.substack.com/',\n",
       " 'https://glennloury.substack.com/',\n",
       " 'https://cripnews.substack.com/',\n",
       " 'https://charlotteledger.substack.com/',\n",
       " 'https://pomp.substack.com/',\n",
       " 'https://takecontrol.substack.com/',\n",
       " 'https://dianabutlerbass.substack.com/',\n",
       " 'https://capitolaccount.substack.com/',\n",
       " 'https://gossiptime.substack.com/',\n",
       " 'https://gideons.substack.com/',\n",
       " 'https://fullstackeconomics.substack.com/',\n",
       " 'https://fictionistas.substack.com/',\n",
       " 'https://hilltophoops.substack.com/',\n",
       " 'https://emilywrites.substack.com/',\n",
       " 'https://pricetheory.substack.com/',\n",
       " 'https://marcwatkins.substack.com/',\n",
       " 'https://griefbacon.substack.com/',\n",
       " 'https://avichawla.substack.com/',\n",
       " 'https://carlalallimusic.substack.com/',\n",
       " 'https://mattmiller.substack.com/',\n",
       " 'https://rossbarkan.substack.com/',\n",
       " 'https://backofmind.substack.com/',\n",
       " 'https://harrymarkle.substack.com/',\n",
       " 'https://snyder.substack.com/',\n",
       " 'https://danrafael.substack.com/',\n",
       " 'https://status.substack.com/',\n",
       " 'https://douglewin.substack.com/',\n",
       " 'https://laurathomas.substack.com/',\n",
       " 'https://chrisgeidner.substack.com/',\n",
       " 'https://elifshafak.substack.com/',\n",
       " 'https://knicksfilmschool.substack.com/',\n",
       " 'https://alislagle.substack.com/',\n",
       " 'https://drtenpenny.substack.com/',\n",
       " 'https://culturcidal.substack.com/',\n",
       " 'https://marytrump.substack.com/',\n",
       " 'https://defaultfriend.substack.com/',\n",
       " 'https://walkingtheworld.substack.com/',\n",
       " 'https://szmm.substack.com/',\n",
       " 'https://gizbrasil.substack.com/',\n",
       " 'https://bethfelkerjones.substack.com/',\n",
       " 'https://rogerpielkejr.substack.com/',\n",
       " 'https://thebookerprizes.substack.com/',\n",
       " 'https://huw.substack.com/',\n",
       " 'https://everydayhate.substack.com/',\n",
       " 'https://smotus.substack.com/',\n",
       " 'https://richardhanania.substack.com/',\n",
       " 'https://smokeempodcast.substack.com/',\n",
       " 'https://jimhightower.substack.com/',\n",
       " 'https://newworkinphilosophy.substack.com/',\n",
       " 'https://lastweekinai.substack.com/',\n",
       " 'https://signorile.substack.com/',\n",
       " 'https://pjvogt.substack.com/',\n",
       " 'https://bigtechnology.substack.com/',\n",
       " 'https://energymixweekender.substack.com/',\n",
       " 'https://chriselliotts.substack.com/',\n",
       " 'https://semianalysis.substack.com/',\n",
       " 'https://tidyfirst.substack.com/',\n",
       " 'https://bowtiedtetra.substack.com/',\n",
       " 'https://salonium.substack.com/',\n",
       " 'https://adambienkov.substack.com/',\n",
       " 'https://libertyrpf.substack.com/',\n",
       " 'https://specialto.substack.com/',\n",
       " 'https://landdesk.substack.com/',\n",
       " 'https://lifeisasacredtext.substack.com/',\n",
       " 'https://chinai.substack.com/',\n",
       " 'https://inktober.substack.com/',\n",
       " 'https://amyodell.substack.com/',\n",
       " 'https://adamgrant.substack.com/',\n",
       " 'https://aarthisriramshow.substack.com/',\n",
       " 'https://evavlaardingerbroek.substack.com/',\n",
       " 'https://larasec.substack.com/',\n",
       " 'https://culturallyenough.substack.com/',\n",
       " 'https://freddiedeboer.substack.com/',\n",
       " 'https://ziller.substack.com/',\n",
       " 'https://awards.substack.com/',\n",
       " 'https://radicle.substack.com/',\n",
       " 'https://davidrozado.substack.com/',\n",
       " 'https://annieduke.substack.com/',\n",
       " 'https://runningforresilience.substack.com/',\n",
       " 'https://smdanler.substack.com/',\n",
       " 'https://yummytoddlerfood.substack.com/',\n",
       " 'https://jamesfell.substack.com/',\n",
       " 'https://gamediscoverability.substack.com/',\n",
       " 'https://thehandbasket.substack.com/',\n",
       " 'https://substack.com/book',\n",
       " 'https://astralcodexten.substack.com/',\n",
       " 'https://carolinagelen.substack.com/',\n",
       " 'https://karenkingston.substack.com/',\n",
       " 'https://thespirits.substack.com/',\n",
       " 'https://healthypets.substack.com/',\n",
       " 'https://theisolationjournals.substack.com/',\n",
       " 'https://rozenberg.substack.com/',\n",
       " 'https://farrah.substack.com/',\n",
       " 'https://betonit.substack.com/',\n",
       " 'https://cegatesc.substack.com/',\n",
       " 'https://newpublic.substack.com/',\n",
       " 'https://danielgordis.substack.com/',\n",
       " 'https://qualitycompounding.substack.com/',\n",
       " 'https://fx.substack.com/',\n",
       " 'https://ivararpi.substack.com/',\n",
       " 'https://emilymariko.substack.com/',\n",
       " 'https://michaelbhorn.substack.com/',\n",
       " 'https://bitstobrands.substack.com/',\n",
       " 'https://asiasentinel.substack.com/',\n",
       " 'https://jimmysong.substack.com/',\n",
       " 'https://tedgioia.substack.com/',\n",
       " 'https://daandelman.substack.com/',\n",
       " 'https://asawinstanley.substack.com/',\n",
       " 'https://technosapiens.substack.com/',\n",
       " 'https://rishad.substack.com/',\n",
       " 'https://yungpueblo.substack.com/',\n",
       " 'https://theemergingvc.substack.com/',\n",
       " 'https://brownhistory.substack.com/',\n",
       " 'https://disabilitydebrief.substack.com/',\n",
       " 'https://writinghacks.substack.com/',\n",
       " 'https://suzannemoore.substack.com/',\n",
       " 'https://normanfinkelstein.substack.com/',\n",
       " 'https://brainhealthkitchen.substack.com/',\n",
       " 'https://joeysims.substack.com/',\n",
       " 'https://theboilup.substack.com/',\n",
       " 'https://branko2f7.substack.com/',\n",
       " 'https://fchollet.substack.com/',\n",
       " 'https://weathertiger.substack.com/',\n",
       " 'https://anncoulter.substack.com/',\n",
       " 'https://ofboysandmen.substack.com/',\n",
       " 'https://jeannakadlec.substack.com/',\n",
       " 'https://yrsadaleyward.substack.com/',\n",
       " 'https://delphizero.substack.com/',\n",
       " 'https://rickmorton.substack.com/',\n",
       " 'https://merylnass.substack.com/',\n",
       " 'https://directormoves.substack.com/',\n",
       " 'https://davidjesudason.substack.com/',\n",
       " 'https://stefankorshak.substack.com/',\n",
       " 'https://bobkravitz.substack.com/',\n",
       " 'https://anasalhajjieoa.substack.com/',\n",
       " 'https://constructionphysics.substack.com/',\n",
       " 'https://paullukas.substack.com/',\n",
       " 'https://taibbi.substack.com/',\n",
       " 'https://plotting.substack.com/',\n",
       " 'https://krystalkyleandfriends.substack.com/',\n",
       " 'https://drpippa.substack.com/',\n",
       " 'https://thegradientpub.substack.com/',\n",
       " 'https://tomryan.substack.com/',\n",
       " 'https://platformchronicles.substack.com/',\n",
       " 'https://luciantruscott.substack.com/',\n",
       " 'https://rufo.substack.com/',\n",
       " 'https://lauraloomer.substack.com/',\n",
       " 'https://alexberenson.substack.com/',\n",
       " 'https://disabilityvisibility.substack.com/',\n",
       " 'https://biblioracle.substack.com/',\n",
       " 'https://thedissenter.substack.com/',\n",
       " 'https://ijeomaoluo.substack.com/',\n",
       " 'https://dgardner.substack.com/',\n",
       " 'https://america.substack.com/',\n",
       " 'https://chrishedges.substack.com/',\n",
       " 'https://bmanalysis.substack.com/',\n",
       " 'https://internetprincess.substack.com/',\n",
       " 'https://lagazettegourmande.substack.com/',\n",
       " 'https://ohevie.substack.com/',\n",
       " 'https://adplist.substack.com/',\n",
       " 'https://savour.substack.com/',\n",
       " 'https://assemblycall.substack.com/',\n",
       " 'https://robertreich.substack.com/',\n",
       " 'https://substack.com/ghost',\n",
       " 'https://bbcrussian.substack.com/',\n",
       " 'https://karlstack.substack.com/',\n",
       " 'https://houseofstrauss.substack.com/',\n",
       " 'https://emilysundberg.substack.com/',\n",
       " 'https://golongtd.substack.com/',\n",
       " 'https://alisoneroman.substack.com/',\n",
       " 'https://oneusefulthing.substack.com/',\n",
       " 'https://erininthemorn.substack.com/',\n",
       " 'https://robkhenderson.substack.com/',\n",
       " 'https://emilyoster.substack.com/',\n",
       " 'https://bariweiss.substack.com/',\n",
       " 'https://open.substack.com/pub/ryanlpeterman',\n",
       " 'https://open.substack.com/pub/pragmaticengineer',\n",
       " 'https://open.substack.com/pub/writeforcalifornia',\n",
       " 'https://open.substack.com/pub/whitepaper',\n",
       " 'https://open.substack.com/pub/lenny',\n",
       " 'https://open.substack.com/pub/matthewyglesias',\n",
       " 'https://open.substack.com/pub/garbageday',\n",
       " 'https://open.substack.com/pub/sebastianraschka',\n",
       " 'https://open.substack.com/pub/bytebytego',\n",
       " 'https://substack.com/home/post/p-139198331',\n",
       " 'https://substack.com/home/post/p-139615284',\n",
       " 'https://nancyj.substack.com/p/breakfast-champions',\n",
       " 'https://thecontender.substack.com/p/sartorial-resolutions',\n",
       " 'https://mygaia.substack.com/p/fern-moss',\n",
       " 'https://substack.com/home/post/p-139697957',\n",
       " 'https://substack.com/home/post/p-139438103',\n",
       " 'https://suzansfieldnotes.substack.com/p/incompetent-leaders',\n",
       " 'https://substack.com/home/post/p-139437256',\n",
       " 'https://substack.com/home/post/p-139466095',\n",
       " 'https://billycarpenter.substack.com/p/scouting-victor-osimhen',\n",
       " 'https://dianefrancis.substack.com/p/next-for-ukraine',\n",
       " 'https://anooshasyed.substack.com/p/my-collage-process',\n",
       " 'https://thecaketoonist.substack.com/p/quick-stollen-kisses',\n",
       " 'https://samdelaney.substack.com/p/consistency-is-boring',\n",
       " 'https://alexandradudley.substack.com/p/cranberry-almond-tart',\n",
       " 'https://crockpotting.substack.com/p/garbanzos-repollo-carrilleras',\n",
       " 'https://annieridout.substack.com/p/change-of-plan',\n",
       " 'https://charlesschifano.substack.com/p/the-solitary-creator',\n",
       " 'https://intercalationstation.substack.com/p/november-2023-roundup',\n",
       " 'https://lorenzoippoliti.substack.com/p/commento-settimanale-a75',\n",
       " 'https://jacobbartlett.substack.com/p/localisation-in-xcode-15',\n",
       " 'https://cerridwenscauldron.substack.com/p/a-victorian-postmistress-tales',\n",
       " 'https://latinamericadailybriefing.substack.com/p/milei-travels-to-us',\n",
       " 'https://elizabethminchilli.substack.com/p/frascarelli-umbrian-pasta-truffles',\n",
       " 'https://uncomfortableconversations.substack.com/p/peter-mccormack-digital-currencies',\n",
       " 'https://fakenous.substack.com/p/beethoven-vs-baby-shark',\n",
       " 'https://sylviavlinsteadt.substack.com/p/hares-on-the-mountain',\n",
       " 'https://jessicadefino.substack.com/p/beauty-editor-skincare-routines',\n",
       " 'https://purpleinsider.substack.com/p/jeffersons-return-requires-another',\n",
       " 'https://vincemancini.substack.com/p/theyre-killing-our-memories',\n",
       " 'https://whattocook.substack.com/p/harissa-bolognese-spaghetti-squash',\n",
       " 'https://chamath.substack.com/p/the-global-energy-transition',\n",
       " 'https://productledseo.substack.com/p/will-googles-sge-launch',\n",
       " 'https://sarahwilson.substack.com/p/some-contemporary-memesnthings-that',\n",
       " 'https://jilloutside.substack.com/p/the-world-you-love',\n",
       " 'https://tapinas.substack.com/p/rytas-staselis-skaidrinam-prienai',\n",
       " 'https://robertglazer.substack.com/p/charlie-munger-warren-buffett',\n",
       " 'https://latinamericadailybriefing.substack.com/p/observers-question-venezuela-referendum',\n",
       " 'https://dadastrain.substack.com/p/bklyn-sounds-126202312122023-harry',\n",
       " 'https://joshlieb.substack.com/p/cartoon-six-four-seven',\n",
       " 'https://hanifkureishi.substack.com/p/autumn-writing-competitions-winners',\n",
       " 'https://carlalallimusic.substack.com/p/nutritional-yeast-vinaigrette-is',\n",
       " 'https://nakedemperor.substack.com/',\n",
       " 'https://mcrumps.substack.com/',\n",
       " 'https://treetalk.substack.com/p/mistletoe-tracking-the-festive-flora',\n",
       " 'https://laurentfrancois.substack.com/p/geo-anonymes-contre-pseudo-realites',\n",
       " 'https://galepooley.substack.com/p/home-alone-with-time-prices',\n",
       " 'https://marioalegre.substack.com/p/godzilla-minus-one-critica-resena',\n",
       " 'https://southlandtribune.substack.com/p/the-southland-tribune-edition-189',\n",
       " 'https://virginiasolesmith.substack.com/p/leanne-brown-good-enough-cooking',\n",
       " 'https://offtolunch.substack.com/p/self-driving-cars-and-wayve',\n",
       " 'https://jimzub.substack.com/p/zubby-newsletter-38-deathtraps-and',\n",
       " 'https://jessica.substack.com/p/abortion-every-day-12623-5b3',\n",
       " 'https://jeffgoins.substack.com/p/life-is-a-dream-b97',\n",
       " 'https://nextyearincleveland.substack.com/p/cleveland-guardians-prospect-report-d42',\n",
       " 'https://trusttheevidence.substack.com/p/the-hallett-inquiry-the-inquirys',\n",
       " 'https://pricepoint.substack.com/p/price-point-041-peak-performance',\n",
       " 'https://249x.substack.com/p/humanoids-and-yet-more-humanoids',\n",
       " 'https://niallharbison.substack.com/p/2023-saving-25000-street-dogs',\n",
       " 'https://bradthomasparsons.substack.com/p/encore-holiday-special-the-santa',\n",
       " 'https://artistmorning.substack.com/p/345-magic-before-your-eyes',\n",
       " 'https://betonit.substack.com/p/critical-comments-on-anarcho-capitalism',\n",
       " 'https://ziller.substack.com/p/zion-williamson-and-the-pelicans',\n",
       " 'https://chriscillizza.substack.com/p/they-call-me-professor-cillizza',\n",
       " 'https://jennifersavrankelly.substack.com/p/hanging-with-writer-friends-counts',\n",
       " 'https://curmudgucation.substack.com/p/sarasota-school-board-should-not',\n",
       " 'https://charlotteledger.substack.com/p/neighbors-upset-over-plans-to-build',\n",
       " 'https://matheusdesouza.substack.com/p/passageiro-59-vivendo-como-um-artista',\n",
       " 'https://baerinmind.substack.com/p/baer-in-mind-december-8-2023',\n",
       " 'https://golongtd.substack.com/p/part-i-will-the-minnesota-vikings',\n",
       " 'https://libertylensecon.substack.com/p/the-silent-revolution-free-speech-8cb',\n",
       " 'https://adjacentpossible.substack.com/p/writing-at-the-speed-of-thought',\n",
       " 'https://substack.com/redirect/19d40f54-a758-4393-b443-d7aa23ac7e0d?j=eyJ1IjoiMjhnbDAzIn0.NIHwKCO0ROIGz2tdzvLZrNNt4dIfbrp3OGf1L08t5_A',\n",
       " 'https://cronesandwich.substack.com/',\n",
       " 'https://zacharyzane.substack.com/']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://krystalkyleandfriends.substack.com/',\n",
       " 'https://chamath.substack.com',\n",
       " 'https://chrishedges.substack.com/',\n",
       " 'https://gingerriver.substack.com/',\n",
       " 'https://jessicadefino.substack.com',\n",
       " 'https://edwardsnowden.substack.com/',\n",
       " 'https://intercalationstation.substack.com',\n",
       " 'https://vincemancini.substack.com',\n",
       " 'https://tatlondon.substack.com/',\n",
       " 'https://astralcodexten.substack.com/',\n",
       " 'https://tedgioia.substack.com/',\n",
       " 'https://quoththeraven.substack.com/',\n",
       " 'https://landdesk.substack.com/',\n",
       " 'https://smokeempodcast.substack.com/',\n",
       " 'https://davidrozado.substack.com/',\n",
       " 'https://joshlieb.substack.com',\n",
       " 'https://salonium.substack.com/',\n",
       " 'https://lordfed.substack.com/',\n",
       " 'https://annebyrn.substack.com/',\n",
       " 'https://gideons.substack.com/',\n",
       " 'https://managingeditor.substack.com/',\n",
       " 'https://zacharyzane.substack.com/',\n",
       " 'https://joshbrake.substack.com/',\n",
       " 'https://snyder.substack.com/',\n",
       " 'https://theupandup.substack.com/',\n",
       " 'https://specialto.substack.com/',\n",
       " 'https://rufo.substack.com/',\n",
       " 'https://thebookerprizes.substack.com/',\n",
       " 'https://robertglazer.substack.com',\n",
       " 'https://marytrump.substack.com/',\n",
       " 'https://eugenesrobinson.substack.com/',\n",
       " 'https://robertreich.substack.com/',\n",
       " 'https://dusttodigital.substack.com/',\n",
       " 'https://billycarpenter.substack.com',\n",
       " 'https://coloradomedia.substack.com/',\n",
       " 'https://indignity.substack.com/',\n",
       " 'https://freddiedeboer.substack.com/',\n",
       " 'https://garyrosenblatt.substack.com/',\n",
       " 'https://lauradodsworth.substack.com/',\n",
       " 'https://stainedpagenews.substack.com/',\n",
       " 'https://importai.substack.com/',\n",
       " 'https://open.substack.com/pub/garbageday',\n",
       " 'https://anncoulter.substack.com/',\n",
       " 'https://zeynep.substack.com/',\n",
       " 'https://alexberenson.substack.com/',\n",
       " 'https://open.substack.com/pub/bytebytego',\n",
       " 'https://smotus.substack.com/',\n",
       " 'https://twitterfiles.substack.com/',\n",
       " 'https://nakedemperor.substack.com/',\n",
       " 'https://katecasey.substack.com/',\n",
       " 'https://dinneralovestory.substack.com/',\n",
       " 'https://alisoneroman.substack.com/',\n",
       " 'https://traditionsanity.substack.com/',\n",
       " 'https://tidyfirst.substack.com/',\n",
       " 'https://matheusdesouza.substack.com',\n",
       " 'https://thedissenter.substack.com/',\n",
       " 'https://houseofstrauss.substack.com/',\n",
       " 'https://nextyearincleveland.substack.com',\n",
       " 'https://chrislatray.substack.com/',\n",
       " 'https://carlalallimusic.substack.com',\n",
       " 'https://adamgrant.substack.com/',\n",
       " 'https://savour.substack.com/',\n",
       " 'https://mattmiller.substack.com/',\n",
       " 'https://gptzero.substack.com/',\n",
       " 'https://biblioracle.substack.com/',\n",
       " 'https://whattocook.substack.com',\n",
       " 'https://betonit.substack.com',\n",
       " 'https://lastweekinai.substack.com/',\n",
       " 'https://danrafael.substack.com/',\n",
       " 'https://glennloury.substack.com/',\n",
       " 'https://gizbrasil.substack.com/',\n",
       " 'https://disabilityvisibility.substack.com/',\n",
       " 'https://dianefrancis.substack.com',\n",
       " 'https://theisolationjournals.substack.com/',\n",
       " 'https://bizzarodevs.substack.com/',\n",
       " 'https://thehockeywriters.substack.com/',\n",
       " 'https://adjacentpossible.substack.com',\n",
       " 'https://ohevie.substack.com/',\n",
       " 'https://charlotteledger.substack.com',\n",
       " 'https://paullukas.substack.com/',\n",
       " 'https://niallharbison.substack.com',\n",
       " 'https://podcastthenewsletter.substack.com/',\n",
       " 'https://notdrinkingpoison.substack.com/',\n",
       " 'https://fullstackeconomics.substack.com/',\n",
       " 'https://gamediscoverability.substack.com/',\n",
       " 'https://jimhightower.substack.com/',\n",
       " 'https://newpublic.substack.com/',\n",
       " 'https://uncomfortableconversations.substack.com',\n",
       " 'https://pomp.substack.com/',\n",
       " 'https://constructionphysics.substack.com/',\n",
       " 'https://bariweiss.substack.com/',\n",
       " 'https://rozenberg.substack.com/',\n",
       " 'https://natesilver.substack.com/',\n",
       " 'https://sarahwilson.substack.com',\n",
       " 'https://offtolunch.substack.com',\n",
       " 'https://steady.substack.com/',\n",
       " 'https://endtimes.substack.com/',\n",
       " 'https://lorenzoippoliti.substack.com',\n",
       " 'https://qualitycompounding.substack.com/',\n",
       " 'https://thecaketoonist.substack.com',\n",
       " 'https://read.substack.com/',\n",
       " 'https://exxonknews.substack.com/',\n",
       " 'https://yungpueblo.substack.com/',\n",
       " 'https://open.substack.com/pub/pragmaticengineer',\n",
       " 'https://hanifkureishi.substack.com',\n",
       " 'https://knicksfilmschool.substack.com/',\n",
       " 'https://equalityalec.substack.com/',\n",
       " 'https://hotglobe.substack.com/',\n",
       " 'https://artistmorning.substack.com',\n",
       " 'https://normanfinkelstein.substack.com/',\n",
       " 'https://danielgordis.substack.com/',\n",
       " 'https://jessica.substack.com/',\n",
       " 'https://emilyoster.substack.com/',\n",
       " 'https://samdelaney.substack.com',\n",
       " 'https://chamath.substack.com/',\n",
       " 'https://karenkingston.substack.com/',\n",
       " 'https://claireberlinski.substack.com/',\n",
       " 'https://open.substack.com/pub/ryanlpeterman',\n",
       " 'https://pjvogt.substack.com/',\n",
       " 'https://interconnect.substack.com/',\n",
       " 'https://femchaospod.substack.com/',\n",
       " 'https://open.substack.com/pub/lenny',\n",
       " 'https://szmm.substack.com/',\n",
       " 'https://fakenous.substack.com',\n",
       " 'https://radicle.substack.com/',\n",
       " 'https://thepillar.substack.com/',\n",
       " 'https://awards.substack.com/',\n",
       " 'https://brainhealthkitchen.substack.com/',\n",
       " 'https://rogerpielkejr.substack.com/',\n",
       " 'https://marcwatkins.substack.com/',\n",
       " 'https://bradthomasparsons.substack.com',\n",
       " 'https://andjelicaaa.substack.com/',\n",
       " 'https://oneusefulthing.substack.com/',\n",
       " 'https://reviewcanada.substack.com/',\n",
       " 'https://codeconfessions.substack.com/',\n",
       " 'https://thecontender.substack.com/',\n",
       " 'https://carlalallimusic.substack.com/',\n",
       " 'https://fx.substack.com/',\n",
       " 'https://jimmysong.substack.com/',\n",
       " 'https://yrsadaleyward.substack.com/',\n",
       " 'https://richardhanania.substack.com/',\n",
       " 'https://bobsturm.substack.com/',\n",
       " 'https://suzansfieldnotes.substack.com',\n",
       " 'https://chriselliotts.substack.com/',\n",
       " 'https://fakephdinvestigator.substack.com/',\n",
       " 'https://evavlaardingerbroek.substack.com/',\n",
       " 'https://amyodell.substack.com/',\n",
       " 'https://thecatholicfeminist.substack.com/',\n",
       " 'https://lagazettegourmande.substack.com/',\n",
       " 'https://annehelen.substack.com/',\n",
       " 'https://trusttheevidence.substack.com',\n",
       " 'https://theupheaval.substack.com/',\n",
       " 'https://tapinas.substack.com',\n",
       " 'https://christianlorentzen.substack.com/',\n",
       " 'https://tomryan.substack.com/',\n",
       " 'http://harkaway.substack.com/',\n",
       " 'https://libertylensecon.substack.com',\n",
       " 'https://nancyj.substack.com/',\n",
       " 'https://paperalfa.substack.com/',\n",
       " 'https://statuskuo.substack.com/',\n",
       " 'https://smdanler.substack.com/',\n",
       " 'https://aarthisriramshow.substack.com/',\n",
       " 'https://golongtd.substack.com',\n",
       " 'https://pickandrollau.substack.com/',\n",
       " 'https://robertpjones.substack.com/',\n",
       " 'https://southlandtribune.substack.com',\n",
       " 'https://truehoop.substack.com/',\n",
       " 'https://thewritersalmanac.substack.com/',\n",
       " 'https://rwmalonemd.substack.com/',\n",
       " 'https://thecontender.substack.com',\n",
       " 'https://asiasentinel.substack.com/',\n",
       " 'https://basu.substack.com/',\n",
       " 'https://suzannemoore.substack.com/',\n",
       " 'https://whyisthisinteresting.substack.com/',\n",
       " 'https://chrisgeidner.substack.com/',\n",
       " 'https://andrewlawton.substack.com/',\n",
       " 'https://sheriffs.substack.com/',\n",
       " 'https://pricetheory.substack.com/',\n",
       " 'https://thehyphen.substack.com/',\n",
       " 'https://adambienkov.substack.com/',\n",
       " 'https://jennifersavrankelly.substack.com',\n",
       " 'https://rickmorton.substack.com/',\n",
       " 'https://open.substack.com/pub/sebastianraschka',\n",
       " 'https://virginiasolesmith.substack.com',\n",
       " 'https://dianabutlerbass.substack.com/',\n",
       " 'https://griefbacon.substack.com/',\n",
       " 'https://susanality.substack.com/',\n",
       " 'https://emilysundberg.substack.com/',\n",
       " 'https://stateofai.substack.com/',\n",
       " 'https://cegatesc.substack.com/',\n",
       " 'https://takecontrol.substack.com/',\n",
       " 'https://adplist.substack.com/',\n",
       " 'https://cerridwenscauldron.substack.com',\n",
       " 'https://walkingtheworld.substack.com/',\n",
       " 'https://jamesfell.substack.com/',\n",
       " 'https://stefankorshak.substack.com/',\n",
       " 'https://platformchronicles.substack.com/',\n",
       " 'https://marioalegre.substack.com',\n",
       " 'https://taibbi.substack.com/',\n",
       " 'https://joeysims.substack.com/',\n",
       " 'https://inktober.substack.com/',\n",
       " 'https://mygaia.substack.com',\n",
       " 'https://bigtechnology.substack.com/',\n",
       " 'https://lifeisasacredtext.substack.com/',\n",
       " 'https://asawinstanley.substack.com/',\n",
       " 'https://petition.substack.com/',\n",
       " 'https://carolinagelen.substack.com/',\n",
       " 'https://dgardner.substack.com/',\n",
       " 'https://wecanfixit.substack.com/',\n",
       " 'https://eyelashroaming.substack.com/',\n",
       " 'https://ismatu.substack.com/',\n",
       " 'https://danperry.substack.com/',\n",
       " 'https://eamonnbrennan.substack.com/',\n",
       " 'https://bethfelkerjones.substack.com/',\n",
       " 'https://ijeomaoluo.substack.com/',\n",
       " 'https://fallows.substack.com/',\n",
       " 'https://michaelbhorn.substack.com/',\n",
       " 'https://artificialintelligencemadesimple.substack.com/',\n",
       " 'https://gossiptime.substack.com/',\n",
       " 'https://thespirits.substack.com/',\n",
       " 'https://pricepoint.substack.com',\n",
       " 'https://arimelber.substack.com/',\n",
       " 'https://internetprincess.substack.com/',\n",
       " 'https://emilymcdowell.substack.com/',\n",
       " 'https://jeannakadlec.substack.com/',\n",
       " 'https://lg.substack.com/',\n",
       " 'https://backofmind.substack.com/',\n",
       " 'https://culturallyenough.substack.com/',\n",
       " 'https://drtenpenny.substack.com/',\n",
       " 'https://maggiesmith.substack.com/',\n",
       " 'https://writinghacks.substack.com/',\n",
       " 'https://libertyrpf.substack.com/',\n",
       " 'https://bowtiedtetra.substack.com/',\n",
       " 'https://cronesandwich.substack.com/',\n",
       " 'https://betonit.substack.com/',\n",
       " 'https://insidethenewsroom.substack.com/',\n",
       " 'https://gentlefoods.substack.com/',\n",
       " 'https://branko2f7.substack.com/',\n",
       " 'https://crockpotting.substack.com',\n",
       " 'https://drpippa.substack.com/',\n",
       " 'https://rossbarkan.substack.com/',\n",
       " 'https://lauraloomer.substack.com/',\n",
       " 'https://galepooley.substack.com',\n",
       " 'https://kareem.substack.com/',\n",
       " 'https://jimmydoom.substack.com/',\n",
       " 'https://bitstobrands.substack.com/',\n",
       " 'https://capitolaccount.substack.com/',\n",
       " 'https://theuconnfastbreak.substack.com/',\n",
       " 'https://katz.substack.com/',\n",
       " 'https://helenlewis.substack.com/',\n",
       " 'https://weathertiger.substack.com/',\n",
       " 'https://laurentfrancois.substack.com',\n",
       " 'https://luciantruscott.substack.com/',\n",
       " 'https://signorile.substack.com/',\n",
       " 'https://golongtd.substack.com/',\n",
       " 'https://sylviavlinsteadt.substack.com',\n",
       " 'https://rishad.substack.com/',\n",
       " 'https://adamset.substack.com/',\n",
       " 'https://newsnotnoisejessicayellin.substack.com/',\n",
       " 'https://jakobnielsenphd.substack.com/',\n",
       " 'https://annieduke.substack.com/',\n",
       " 'https://radleybalko.substack.com/',\n",
       " 'https://rychappell.substack.com/',\n",
       " 'https://anooshasyed.substack.com',\n",
       " 'https://merylnass.substack.com/',\n",
       " 'https://larasec.substack.com/',\n",
       " 'https://technosapiens.substack.com/',\n",
       " 'https://lauriestone.substack.com/',\n",
       " 'https://chinai.substack.com/',\n",
       " 'https://elifshafak.substack.com/',\n",
       " 'https://defaultfriend.substack.com/',\n",
       " 'https://karenswallowprior.substack.com/',\n",
       " 'https://disabilitydebrief.substack.com/',\n",
       " 'https://bmanalysis.substack.com/',\n",
       " 'https://wondertools.substack.com/',\n",
       " 'https://tamaradler.substack.com/',\n",
       " 'https://newworkinphilosophy.substack.com/',\n",
       " 'https://annieridout.substack.com',\n",
       " 'https://runningforresilience.substack.com/',\n",
       " 'https://jeffgoins.substack.com',\n",
       " 'https://open.substack.com/pub/whitepaper',\n",
       " 'https://open.substack.com/pub/writeforcalifornia',\n",
       " 'https://ivararpi.substack.com/',\n",
       " 'https://eatgordaeat.substack.com/',\n",
       " 'https://aseanwonk.substack.com/',\n",
       " 'https://jilloutside.substack.com',\n",
       " 'https://fasterplease.substack.com/',\n",
       " 'https://emilymariko.substack.com/',\n",
       " 'https://marissarothkopf.substack.com/',\n",
       " 'https://avichawla.substack.com/',\n",
       " 'https://kcsn.substack.com/',\n",
       " 'https://robkhenderson.substack.com/',\n",
       " 'https://curmudgucation.substack.com',\n",
       " 'https://mollyknight.substack.com/',\n",
       " 'https://johncanzano.substack.com/',\n",
       " 'https://elizabethminchilli.substack.com',\n",
       " 'https://brownhistory.substack.com/',\n",
       " 'https://dellavolpe.substack.com/',\n",
       " 'https://johannadrucker.substack.com/',\n",
       " 'https://emilywrites.substack.com/',\n",
       " 'https://erininthemorn.substack.com/',\n",
       " 'https://anasalhajjieoa.substack.com/',\n",
       " 'https://bobkravitz.substack.com/',\n",
       " 'https://theclimatebrink.substack.com/',\n",
       " 'https://theemergingvc.substack.com/',\n",
       " 'https://plotting.substack.com/',\n",
       " 'https://artificialcorner.substack.com/',\n",
       " 'https://yummytoddlerfood.substack.com/',\n",
       " 'https://hilltophoops.substack.com/',\n",
       " 'https://figsinwinter.substack.com/',\n",
       " 'https://charlesschifano.substack.com',\n",
       " 'https://matthewgreenglobal.substack.com/',\n",
       " 'https://mcrumps.substack.com/',\n",
       " 'https://themargins.substack.com/',\n",
       " 'https://leefang.substack.com/',\n",
       " 'https://america.substack.com/',\n",
       " 'https://caughtoffside.substack.com/',\n",
       " 'https://ryanmcbeth.substack.com/',\n",
       " 'https://daandelman.substack.com/',\n",
       " 'https://shannanmartin.substack.com/',\n",
       " 'https://theshiftwithsambaker.substack.com/',\n",
       " 'https://delphizero.substack.com/',\n",
       " 'https://laurathomas.substack.com/',\n",
       " 'https://reddmonitor.substack.com/',\n",
       " 'https://huw.substack.com/',\n",
       " 'https://edwest.substack.com/',\n",
       " 'https://open.substack.com/pub/matthewyglesias',\n",
       " 'https://249x.substack.com',\n",
       " 'https://afterschool.substack.com/',\n",
       " 'https://dadastrain.substack.com',\n",
       " 'https://culturcidal.substack.com/',\n",
       " 'https://mattstoller.substack.com/',\n",
       " 'https://theboilup.substack.com/',\n",
       " 'https://jessica.substack.com',\n",
       " 'https://cripnews.substack.com/',\n",
       " 'https://fchollet.substack.com/',\n",
       " 'https://jonn.substack.com/',\n",
       " 'https://latinamericadailybriefing.substack.com',\n",
       " 'https://baerinmind.substack.com',\n",
       " 'https://tomkinstimes.substack.com/',\n",
       " 'https://karlstack.substack.com/',\n",
       " 'https://charlotteledger.substack.com/',\n",
       " 'https://harrymarkle.substack.com/',\n",
       " 'https://davidjesudason.substack.com/',\n",
       " 'https://peterbeinart.substack.com/',\n",
       " 'https://everydayhate.substack.com/',\n",
       " 'https://kjramseywrites.substack.com/',\n",
       " 'https://douglewin.substack.com/',\n",
       " 'https://semianalysis.substack.com/',\n",
       " 'https://thehandbasket.substack.com/',\n",
       " 'https://fictionistas.substack.com/',\n",
       " 'https://farrah.substack.com/',\n",
       " 'https://productledseo.substack.com',\n",
       " 'https://directormoves.substack.com/',\n",
       " 'https://alislagle.substack.com/',\n",
       " 'https://assemblycall.substack.com/',\n",
       " 'https://seymourhersh.substack.com/',\n",
       " 'https://heathercoxrichardson.substack.com/',\n",
       " 'https://stevevladeck.substack.com/',\n",
       " 'https://chriscillizza.substack.com',\n",
       " 'https://alexandradudley.substack.com',\n",
       " 'https://myclimatejourney.substack.com/',\n",
       " 'https://treetalk.substack.com',\n",
       " 'https://wordloaf.substack.com/',\n",
       " 'https://ziller.substack.com/',\n",
       " 'https://energymixweekender.substack.com/',\n",
       " 'https://thegradientpub.substack.com/',\n",
       " 'https://healthypets.substack.com/',\n",
       " 'https://boriquagato.substack.com/',\n",
       " 'https://purpleinsider.substack.com',\n",
       " 'https://loleen.substack.com/',\n",
       " 'https://status.substack.com/',\n",
       " 'https://bbcrussian.substack.com/',\n",
       " 'https://jimzub.substack.com',\n",
       " 'https://techworldwithmilan.substack.com/',\n",
       " 'https://jacobbartlett.substack.com',\n",
       " 'https://nancyj.substack.com',\n",
       " 'https://fathernathan.substack.com/',\n",
       " 'https://ofboysandmen.substack.com/',\n",
       " 'https://anneboyer.substack.com/',\n",
       " 'https://alicebell.substack.com/',\n",
       " 'https://jeffreycarr.substack.com/',\n",
       " 'https://nathanbenaich.substack.com/',\n",
       " 'https://ziller.substack.com']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract only blogs that are substack sites\n",
    "results = [x for x in results if \".substack.com\" in x]\n",
    "\n",
    "# remove any sites that are blogs, we only want the original homepage\n",
    "filtered_results = []\n",
    "for url in results:\n",
    "    if \"/p/\" in url:\n",
    "        filtered_results.append(url.split(\"/p/\")[0])\n",
    "    else:\n",
    "        filtered_results.append(url)\n",
    "\n",
    "# remove duplicates\n",
    "filtered_results = list(set(filtered_results))\n",
    "filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of subscribers for each blog\n",
    "\n",
    "subscriber_counts = []\n",
    "for url in tqdm(filtered_results):\n",
    "    # get subscriber count for the blog\n",
    "    subscribers_count = None\n",
    "    script_element = None\n",
    "    json_data = None\n",
    "    parsed_dict = None\n",
    "    formatted_dict = None\n",
    "    subscribers_count_str = None\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        # Assuming you have the HTML content of the page loaded into BeautifulSoup as 'soup'\n",
    "        # Find the script element containing the JSON data\n",
    "        script_elements = soup.find_all(\"script\")\n",
    "\n",
    "        for script_element in script_elements:\n",
    "            if \"subscribers\" in script_element.text:\n",
    "                break\n",
    "\n",
    "        input_string = script_element.text\n",
    "        \n",
    "        # Find the JSON data within the string\n",
    "        start_index = input_string.find('JSON.parse(') + len('JSON.parse(')\n",
    "        end_index = input_string.rfind(');', start_index)\n",
    "\n",
    "        # Extract the JSON data\n",
    "        json_data = input_string[start_index:end_index].strip()\n",
    "\n",
    "        # Parse the JSON data into a dictionary\n",
    "        parsed_dict = json.loads(json_data)\n",
    "\n",
    "        def extract_value_by_key(dictionary, target_key):\n",
    "            # Check if the target_key is in the current dictionary\n",
    "            if target_key in dictionary:\n",
    "                return dictionary[target_key]\n",
    "            \n",
    "            # If the key is not found, recursively search in nested dictionaries\n",
    "            for key, value in dictionary.items():\n",
    "                if isinstance(value, dict):\n",
    "                    result = extract_value_by_key(value, target_key)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "            \n",
    "            # If the key is not found anywhere in the dictionary, return None\n",
    "            return None\n",
    "\n",
    "        formatted_dict = json.loads(parsed_dict)\n",
    "\n",
    "        target_key = \"rankingDetailFreeSubscriberCount\"\n",
    "        subscriber_str = extract_value_by_key(formatted_dict, target_key)\n",
    "\n",
    "        # Use regular expressions to find the number with commas in the string\n",
    "        match = re.search(r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?', subscriber_str)\n",
    "\n",
    "        if match:\n",
    "            subscribers_count_str = match.group()\n",
    "            # Remove commas and convert to an integer\n",
    "            subscribers_count = int(subscribers_count_str.replace(',', ''))\n",
    "        else:\n",
    "            print(\"No subscribers count found in the string.\")\n",
    "            subscribers_count = None\n",
    "    except Exception as e:\n",
    "        print(\"Error\", e)\n",
    "        subscribers_count = None\n",
    "        # Print the traceback\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback.print_tb(exc_traceback)\n",
    "\n",
    "    subscriber_counts.append(subscribers_count)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://krystalkyleandfriends.substack.com/</td>\n",
       "      <td>44000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://chamath.substack.com</td>\n",
       "      <td>69000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://chrishedges.substack.com/</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://gingerriver.substack.com/</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://jessicadefino.substack.com</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>https://anneboyer.substack.com/</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>https://alicebell.substack.com/</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>https://jeffreycarr.substack.com/</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>https://nathanbenaich.substack.com/</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>https://ziller.substack.com</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            blog  subscribers\n",
       "0    https://krystalkyleandfriends.substack.com/      44000.0\n",
       "1                   https://chamath.substack.com      69000.0\n",
       "2              https://chrishedges.substack.com/      72000.0\n",
       "3              https://gingerriver.substack.com/       5000.0\n",
       "4             https://jessicadefino.substack.com      90000.0\n",
       "..                                           ...          ...\n",
       "379              https://anneboyer.substack.com/       7000.0\n",
       "380              https://alicebell.substack.com/       9000.0\n",
       "381            https://jeffreycarr.substack.com/       3000.0\n",
       "382          https://nathanbenaich.substack.com/      24000.0\n",
       "383                  https://ziller.substack.com      12000.0\n",
       "\n",
       "[384 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble metadata\n",
    "metadata_df = pd.DataFrame()\n",
    "metadata_df['blog'] = filtered_results\n",
    "metadata_df['subscribers'] = subscriber_counts\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://krystalkyleandfriends.substack.com/</td>\n",
       "      <td>44000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://chamath.substack.com</td>\n",
       "      <td>69000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://chrishedges.substack.com/</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://gingerriver.substack.com/</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://jessicadefino.substack.com</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>https://anneboyer.substack.com/</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>https://alicebell.substack.com/</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>https://jeffreycarr.substack.com/</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>https://nathanbenaich.substack.com/</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>https://ziller.substack.com</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            blog  subscribers\n",
       "0    https://krystalkyleandfriends.substack.com/      44000.0\n",
       "1                   https://chamath.substack.com      69000.0\n",
       "2              https://chrishedges.substack.com/      72000.0\n",
       "3              https://gingerriver.substack.com/       5000.0\n",
       "4             https://jessicadefino.substack.com      90000.0\n",
       "..                                           ...          ...\n",
       "379              https://anneboyer.substack.com/       7000.0\n",
       "380              https://alicebell.substack.com/       9000.0\n",
       "381            https://jeffreycarr.substack.com/       3000.0\n",
       "382          https://nathanbenaich.substack.com/      24000.0\n",
       "383                  https://ziller.substack.com      12000.0\n",
       "\n",
       "[309 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove blogs where we could not collect subscriber counts\n",
    "metadata_df = metadata_df.dropna()\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database and save the data\n",
    "db_file = \"substack_database.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Iterate through the DataFrame and insert rows into blog_metadata\n",
    "for index, row in metadata_df.iterrows():\n",
    "    blog_url = row['blog']\n",
    "    subscriber_count = row['subscribers']\n",
    "\n",
    "    # Check if the blog_url already exists in the table\n",
    "    cursor.execute(\"SELECT blog_url FROM blog_metadata WHERE blog_url=?\", (blog_url,))\n",
    "    existing_row = cursor.fetchone()\n",
    "\n",
    "    if not existing_row:\n",
    "        # Insert the row into the table if it doesn't exist\n",
    "        cursor.execute(\"INSERT INTO blog_metadata (blog_url, subscriber_count) VALUES (?, ?)\",\n",
    "                       (blog_url, subscriber_count))\n",
    "\n",
    "# Commit changes and close the database connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Articles From Substack Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 39/309 [1:18:03<6:08:21, 81.86s/it]  "
     ]
    }
   ],
   "source": [
    "# collect all article data for all blogs that we have available.\n",
    "from utils import get_posts_for_blog, get_post_metadata\n",
    "\n",
    "for x in tqdm(range(0, len(metadata_df))):\n",
    "    try:\n",
    "        blog_url = metadata_df.iloc[x]['blog'].replace(\".com/\", \".com\")\n",
    "        blog_subscribers = metadata_df.iloc[x]['subscribers'] \n",
    "\n",
    "\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Retrieve all canonical_urls from the article_data table - need to check if we've already collected articles for this blog. \n",
    "        cursor.execute(\"SELECT canonical_url FROM article_data\")\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Close the database connection\n",
    "        conn.close()\n",
    "\n",
    "        # Extract the first part of each URL and add \".com\" back to it\n",
    "        unique_domains = set()\n",
    "        for result in results:\n",
    "            url = result[0]\n",
    "            domain_parts = url.split('.com')\n",
    "            if len(domain_parts) > 0:\n",
    "                unique_domains.append(domain_parts[0] + \".com\")\n",
    "\n",
    "        # if the blog has already been connected, dont collect the data it\n",
    "        if blog_url in unique_domains:\n",
    "            pass\n",
    "        else:\n",
    "            post_data = get_posts_for_blog(blog_url)\n",
    "            \n",
    "            conn = sqlite3.connect(db_file)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Iterate through the DataFrame and insert rows into article_data\n",
    "            for index, row in post_data.iterrows():\n",
    "                title = row['title']\n",
    "                audience = row['audience']\n",
    "                canonical_url = row['canonical_url']\n",
    "                description = row['description']\n",
    "                truncated_body_text = row['truncated_body_text']\n",
    "                wordcount = row['wordcount']\n",
    "                reaction_count = row['reaction_count']\n",
    "                comment_count = row['comment_count']\n",
    "                post_date = row['post_date']\n",
    "\n",
    "                # Check if the canonical_url already exists in the table\n",
    "                cursor.execute(\"SELECT canonical_url FROM article_data WHERE canonical_url=?\", (canonical_url,))\n",
    "                existing_row = cursor.fetchone()\n",
    "\n",
    "                if not existing_row:\n",
    "                    # Insert the row into the table if it doesn't exist\n",
    "                    cursor.execute(\"INSERT INTO article_data (title, audience, canonical_url, description, truncated_body_text, wordcount, reaction_count, comment_count, post_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                                (title, audience, canonical_url, description, truncated_body_text, wordcount, reaction_count, comment_count, post_date))\n",
    "\n",
    "            # Commit changes and close the database connection\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Articles, Calculate Metrics\n",
    "After identifying article links, we still need to download the article text and calculate some metrics we'll use later for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6547 [00:00<?, ?it/s]/Users/hansenhan/substack_scraper/utils.py:289: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 289 of the file /Users/hansenhan/substack_scraper/utils.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  r = requests.get(post_url)\n",
      "/Users/hansenhan/substack_scraper/utils.py:297: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 297 of the file /Users/hansenhan/substack_scraper/utils.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  # get the counts of the elements\n",
      "100%|| 6547/6547 [17:56<00:00,  6.08it/s]  \n"
     ]
    }
   ],
   "source": [
    "# add more metadata to all article data\n",
    "from utils import get_post_metadata_from_url\n",
    "\n",
    "def insert_results_into_db(db_path, url, results):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Convert the 'tokens_results' dictionary to a JSON string\n",
    "    tokens_results_json = json.dumps(results[\"tokens_results\"])\n",
    "\n",
    "    # Update the database row with the obtained results\n",
    "    cursor.execute('''\n",
    "        UPDATE article_data\n",
    "        SET\n",
    "            p_elem_counts = ?,\n",
    "            img_elem_counts = ?,\n",
    "            a_elem_counts = ?,\n",
    "            ul_elem_counts = ?,\n",
    "            li_elem_counts = ?,\n",
    "            video_elem_counts = ?,\n",
    "            br_elem_counts = ?,\n",
    "            tokens = ?,\n",
    "            polarity = ?,\n",
    "            objectivity = ?,\n",
    "            number_of_questions = ?,\n",
    "            fk_grade_level = ?,\n",
    "            gunning_fog_index = ?,\n",
    "            reading_time = ?\n",
    "        WHERE canonical_url = ?\n",
    "    ''', (\n",
    "        results[\"p_elem_counts\"],\n",
    "        results[\"img_elem_counts\"],\n",
    "        results[\"a_elem_counts\"],\n",
    "        results[\"ul_elem_counts\"],\n",
    "        results[\"li_elem_counts\"],\n",
    "        results[\"video_elem_counts\"],\n",
    "        results[\"br_elem_counts\"],\n",
    "        tokens_results_json,\n",
    "        results[\"polarity_results\"],\n",
    "        results[\"objectivity_results\"],\n",
    "        results[\"num_questions_results\"],\n",
    "        results[\"fk_grade_level_results\"],\n",
    "        results[\"gunning_fog_index_results\"],\n",
    "        results[\"reading_time_results\"],\n",
    "        url\n",
    "    ))\n",
    "\n",
    "    # Commit the changes and close the database connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# first, get all rows where metadata hasnt been collected (all the fields are null)\n",
    "db_file = \"substack_database.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT canonical_url\n",
    "    FROM article_data\n",
    "    WHERE polarity IS NULL\n",
    "      AND objectivity IS NULL\n",
    "      AND number_of_questions IS NULL\n",
    "      AND fk_grade_level IS NULL\n",
    "      AND gunning_fog_index IS NULL\n",
    "      AND reading_time IS NULL\n",
    "      AND p_elem_counts IS NULL\n",
    "      AND a_elem_counts IS NULL\n",
    "      AND img_elem_counts IS NULL\n",
    "      AND ul_elem_counts IS NULL\n",
    "      AND li_elem_counts IS NULL\n",
    "      AND video_elem_counts IS NULL\n",
    "      AND br_elem_counts IS NULL\n",
    "      AND tokens IS NULL\n",
    "      AND audience = \"everyone\"\n",
    "''')\n",
    "\n",
    "# Fetch all the rows that match the criteria\n",
    "rows = cursor.fetchall()\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "for row in tqdm(rows):\n",
    "    try:\n",
    "        url = row[0]\n",
    "        results = get_post_metadata_from_url(url)\n",
    "        insert_results_into_db(db_file, url, results)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
