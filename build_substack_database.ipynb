{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Substack Text Database\n",
    "\n",
    "The goal of this is to build a database that we can use to identify interesting keywords, topics, writing patterns, and themes, etc. that are associated with higher levels of subscribers and engagement for writers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries, Setting Up Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "import sys\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = \"substack_database.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the blog_metadata table\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS blog_metadata (\n",
    "        blog_url TEXT PRIMARY KEY,\n",
    "        subscriber_count INTEGER,\n",
    "        public_post_count INTEGER,\n",
    "        private_post_count INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS article_data (\n",
    "        title TEXT,\n",
    "        audience TEXT,\n",
    "        canonical_url TEXT PRIMARY KEY,\n",
    "        description TEXT,\n",
    "        truncated_body_text TEXT,\n",
    "        wordcount INTEGER,\n",
    "        reaction_count INTEGER,\n",
    "        comment_count INTEGER,\n",
    "        post_date TEXT, \n",
    "        polarity REAL,\n",
    "        objectivity REAL,\n",
    "        number_of_questions INTEGER,\n",
    "        fk_grade_level REAL,\n",
    "        gunning_fog_index REAL,\n",
    "        reading_time REAL,\n",
    "        p_elem_counts INTEGER,\n",
    "        a_elem_counts INTEGER,\n",
    "        img_elem_counts INTEGER,\n",
    "        ul_elem_counts INTEGER,\n",
    "        li_elem_counts INTEGER,\n",
    "        video_elem_counts INTEGER,\n",
    "        br_elem_counts INTEGER,\n",
    "        tokens TEXT\n",
    "    )\n",
    "''')\n",
    "\n",
    "\n",
    "# Commit changes and close the database connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Substack Blogs For Scraping\n",
    "Here we identify substack blogs and get the number of subscribers per blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the query and the desired website domain to search\n",
    "# it would be nice to have a better way to do this.... \n",
    "query = \"site:substack.com\"\n",
    "num_results = 1000\n",
    "\n",
    "# Create a list to store the URLs\n",
    "results = []\n",
    "\n",
    "# Perform the Google search and scrape the URLs\n",
    "for url in tqdm(search(query, num_results=num_results)):\n",
    "    results.append(url)\n",
    "    #time.sleep(1) # space out requests so we dont get blocked by google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only blogs that are substack sites\n",
    "results = [x for x in results if \".substack.com\" in x]\n",
    "\n",
    "# remove any sites that are blogs, we only want the original homepage\n",
    "filtered_results = []\n",
    "for url in results:\n",
    "    if \"/p/\" in url:\n",
    "        filtered_results.append(url.split(\"/p/\")[0])\n",
    "    else:\n",
    "        filtered_results.append(url)\n",
    "\n",
    "# remove duplicates\n",
    "filtered_results = list(set(filtered_results))\n",
    "filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of subscribers for each blog\n",
    "\n",
    "subscriber_counts = []\n",
    "for url in tqdm(filtered_results):\n",
    "    # get subscriber count for the blog\n",
    "    subscribers_count = None\n",
    "    script_element = None\n",
    "    json_data = None\n",
    "    parsed_dict = None\n",
    "    formatted_dict = None\n",
    "    subscribers_count_str = None\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        # Assuming you have the HTML content of the page loaded into BeautifulSoup as 'soup'\n",
    "        # Find the script element containing the JSON data\n",
    "        script_elements = soup.find_all(\"script\")\n",
    "\n",
    "        for script_element in script_elements:\n",
    "            if \"subscribers\" in script_element.text:\n",
    "                break\n",
    "\n",
    "        input_string = script_element.text\n",
    "        \n",
    "        # Find the JSON data within the string\n",
    "        start_index = input_string.find('JSON.parse(') + len('JSON.parse(')\n",
    "        end_index = input_string.rfind(');', start_index)\n",
    "\n",
    "        # Extract the JSON data\n",
    "        json_data = input_string[start_index:end_index].strip()\n",
    "\n",
    "        # Parse the JSON data into a dictionary\n",
    "        parsed_dict = json.loads(json_data)\n",
    "\n",
    "        def extract_value_by_key(dictionary, target_key):\n",
    "            # Check if the target_key is in the current dictionary\n",
    "            if target_key in dictionary:\n",
    "                return dictionary[target_key]\n",
    "            \n",
    "            # If the key is not found, recursively search in nested dictionaries\n",
    "            for key, value in dictionary.items():\n",
    "                if isinstance(value, dict):\n",
    "                    result = extract_value_by_key(value, target_key)\n",
    "                    if result is not None:\n",
    "                        return result\n",
    "            \n",
    "            # If the key is not found anywhere in the dictionary, return None\n",
    "            return None\n",
    "\n",
    "        formatted_dict = json.loads(parsed_dict)\n",
    "\n",
    "        target_key = \"rankingDetailFreeSubscriberCount\"\n",
    "        subscriber_str = extract_value_by_key(formatted_dict, target_key)\n",
    "\n",
    "        # Use regular expressions to find the number with commas in the string\n",
    "        match = re.search(r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?', subscriber_str)\n",
    "\n",
    "        if match:\n",
    "            subscribers_count_str = match.group()\n",
    "            # Remove commas and convert to an integer\n",
    "            subscribers_count = int(subscribers_count_str.replace(',', ''))\n",
    "        else:\n",
    "            print(\"No subscribers count found in the string.\")\n",
    "            subscribers_count = None\n",
    "    except Exception as e:\n",
    "        print(\"Error\", e)\n",
    "        subscribers_count = None\n",
    "        # Print the traceback\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback.print_tb(exc_traceback)\n",
    "\n",
    "    subscriber_counts.append(subscribers_count)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://krystalkyleandfriends.substack.com/</td>\n",
       "      <td>44000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://chamath.substack.com</td>\n",
       "      <td>69000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://chrishedges.substack.com/</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://gingerriver.substack.com/</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://jessicadefino.substack.com</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>https://anneboyer.substack.com/</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>https://alicebell.substack.com/</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>https://jeffreycarr.substack.com/</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>https://nathanbenaich.substack.com/</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>https://ziller.substack.com</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            blog  subscribers\n",
       "0    https://krystalkyleandfriends.substack.com/      44000.0\n",
       "1                   https://chamath.substack.com      69000.0\n",
       "2              https://chrishedges.substack.com/      72000.0\n",
       "3              https://gingerriver.substack.com/       5000.0\n",
       "4             https://jessicadefino.substack.com      90000.0\n",
       "..                                           ...          ...\n",
       "379              https://anneboyer.substack.com/       7000.0\n",
       "380              https://alicebell.substack.com/       9000.0\n",
       "381            https://jeffreycarr.substack.com/       3000.0\n",
       "382          https://nathanbenaich.substack.com/      24000.0\n",
       "383                  https://ziller.substack.com      12000.0\n",
       "\n",
       "[384 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble metadata\n",
    "metadata_df = pd.DataFrame()\n",
    "metadata_df['blog'] = filtered_results\n",
    "metadata_df['subscribers'] = subscriber_counts\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog</th>\n",
       "      <th>subscribers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://krystalkyleandfriends.substack.com/</td>\n",
       "      <td>44000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://chamath.substack.com</td>\n",
       "      <td>69000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://chrishedges.substack.com/</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://gingerriver.substack.com/</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://jessicadefino.substack.com</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>https://anneboyer.substack.com/</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>https://alicebell.substack.com/</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>https://jeffreycarr.substack.com/</td>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>https://nathanbenaich.substack.com/</td>\n",
       "      <td>24000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>https://ziller.substack.com</td>\n",
       "      <td>12000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            blog  subscribers\n",
       "0    https://krystalkyleandfriends.substack.com/      44000.0\n",
       "1                   https://chamath.substack.com      69000.0\n",
       "2              https://chrishedges.substack.com/      72000.0\n",
       "3              https://gingerriver.substack.com/       5000.0\n",
       "4             https://jessicadefino.substack.com      90000.0\n",
       "..                                           ...          ...\n",
       "379              https://anneboyer.substack.com/       7000.0\n",
       "380              https://alicebell.substack.com/       9000.0\n",
       "381            https://jeffreycarr.substack.com/       3000.0\n",
       "382          https://nathanbenaich.substack.com/      24000.0\n",
       "383                  https://ziller.substack.com      12000.0\n",
       "\n",
       "[309 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove blogs where we could not collect subscriber counts\n",
    "metadata_df = metadata_df.dropna()\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database and save the data\n",
    "db_file = \"substack_database.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Iterate through the DataFrame and insert rows into blog_metadata\n",
    "for index, row in metadata_df.iterrows():\n",
    "    blog_url = row['blog']\n",
    "    subscriber_count = row['subscribers']\n",
    "\n",
    "    # Check if the blog_url already exists in the table\n",
    "    cursor.execute(\"SELECT blog_url FROM blog_metadata WHERE blog_url=?\", (blog_url,))\n",
    "    existing_row = cursor.fetchone()\n",
    "\n",
    "    if not existing_row:\n",
    "        # Insert the row into the table if it doesn't exist\n",
    "        cursor.execute(\"INSERT INTO blog_metadata (blog_url, subscriber_count) VALUES (?, ?)\",\n",
    "                       (blog_url, subscriber_count))\n",
    "\n",
    "# Commit changes and close the database connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Articles From Substack Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 39/309 [1:18:03<6:08:21, 81.86s/it]  "
     ]
    }
   ],
   "source": [
    "# collect all article data for all blogs that we have available.\n",
    "from utils import get_posts_for_blog, get_post_metadata\n",
    "\n",
    "for x in tqdm(range(0, len(metadata_df))):\n",
    "    try:\n",
    "        blog_url = metadata_df.iloc[x]['blog'].replace(\".com/\", \".com\")\n",
    "        blog_subscribers = metadata_df.iloc[x]['subscribers'] \n",
    "\n",
    "\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Retrieve all canonical_urls from the article_data table - need to check if we've already collected articles for this blog. \n",
    "        cursor.execute(\"SELECT canonical_url FROM article_data\")\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Close the database connection\n",
    "        conn.close()\n",
    "\n",
    "        # Extract the first part of each URL and add \".com\" back to it\n",
    "        unique_domains = set()\n",
    "        for result in results:\n",
    "            url = result[0]\n",
    "            domain_parts = url.split('.com')\n",
    "            if len(domain_parts) > 0:\n",
    "                unique_domains.append(domain_parts[0] + \".com\")\n",
    "\n",
    "        # if the blog has already been connected, dont collect the data it\n",
    "        if blog_url in unique_domains:\n",
    "            pass\n",
    "        else:\n",
    "            post_data = get_posts_for_blog(blog_url)\n",
    "            \n",
    "            conn = sqlite3.connect(db_file)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Iterate through the DataFrame and insert rows into article_data\n",
    "            for index, row in post_data.iterrows():\n",
    "                title = row['title']\n",
    "                audience = row['audience']\n",
    "                canonical_url = row['canonical_url']\n",
    "                description = row['description']\n",
    "                truncated_body_text = row['truncated_body_text']\n",
    "                wordcount = row['wordcount']\n",
    "                reaction_count = row['reaction_count']\n",
    "                comment_count = row['comment_count']\n",
    "                post_date = row['post_date']\n",
    "\n",
    "                # Check if the canonical_url already exists in the table\n",
    "                cursor.execute(\"SELECT canonical_url FROM article_data WHERE canonical_url=?\", (canonical_url,))\n",
    "                existing_row = cursor.fetchone()\n",
    "\n",
    "                if not existing_row:\n",
    "                    # Insert the row into the table if it doesn't exist\n",
    "                    cursor.execute(\"INSERT INTO article_data (title, audience, canonical_url, description, truncated_body_text, wordcount, reaction_count, comment_count, post_date) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "                                (title, audience, canonical_url, description, truncated_body_text, wordcount, reaction_count, comment_count, post_date))\n",
    "\n",
    "            # Commit changes and close the database connection\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Articles, Calculate Metrics\n",
    "After identifying article links, we still need to download the article text and calculate some metrics we'll use later for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6547 [00:00<?, ?it/s]/Users/hansenhan/substack_scraper/utils.py:289: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 289 of the file /Users/hansenhan/substack_scraper/utils.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  r = requests.get(post_url)\n",
      "/Users/hansenhan/substack_scraper/utils.py:297: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 297 of the file /Users/hansenhan/substack_scraper/utils.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  # get the counts of the elements\n",
      "100%|██████████| 6547/6547 [17:56<00:00,  6.08it/s]  \n"
     ]
    }
   ],
   "source": [
    "# add more metadata to all article data\n",
    "from utils import get_post_metadata_from_url\n",
    "\n",
    "def insert_results_into_db(db_path, url, results):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Convert the 'tokens_results' dictionary to a JSON string\n",
    "    tokens_results_json = json.dumps(results[\"tokens_results\"])\n",
    "\n",
    "    # Update the database row with the obtained results\n",
    "    cursor.execute('''\n",
    "        UPDATE article_data\n",
    "        SET\n",
    "            p_elem_counts = ?,\n",
    "            img_elem_counts = ?,\n",
    "            a_elem_counts = ?,\n",
    "            ul_elem_counts = ?,\n",
    "            li_elem_counts = ?,\n",
    "            video_elem_counts = ?,\n",
    "            br_elem_counts = ?,\n",
    "            tokens = ?,\n",
    "            polarity = ?,\n",
    "            objectivity = ?,\n",
    "            number_of_questions = ?,\n",
    "            fk_grade_level = ?,\n",
    "            gunning_fog_index = ?,\n",
    "            reading_time = ?\n",
    "        WHERE canonical_url = ?\n",
    "    ''', (\n",
    "        results[\"p_elem_counts\"],\n",
    "        results[\"img_elem_counts\"],\n",
    "        results[\"a_elem_counts\"],\n",
    "        results[\"ul_elem_counts\"],\n",
    "        results[\"li_elem_counts\"],\n",
    "        results[\"video_elem_counts\"],\n",
    "        results[\"br_elem_counts\"],\n",
    "        tokens_results_json,\n",
    "        results[\"polarity_results\"],\n",
    "        results[\"objectivity_results\"],\n",
    "        results[\"num_questions_results\"],\n",
    "        results[\"fk_grade_level_results\"],\n",
    "        results[\"gunning_fog_index_results\"],\n",
    "        results[\"reading_time_results\"],\n",
    "        url\n",
    "    ))\n",
    "\n",
    "    # Commit the changes and close the database connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# first, get all rows where metadata hasnt been collected (all the fields are null)\n",
    "db_file = \"substack_database.db\"\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT canonical_url\n",
    "    FROM article_data\n",
    "    WHERE polarity IS NULL\n",
    "      AND objectivity IS NULL\n",
    "      AND number_of_questions IS NULL\n",
    "      AND fk_grade_level IS NULL\n",
    "      AND gunning_fog_index IS NULL\n",
    "      AND reading_time IS NULL\n",
    "      AND p_elem_counts IS NULL\n",
    "      AND a_elem_counts IS NULL\n",
    "      AND img_elem_counts IS NULL\n",
    "      AND ul_elem_counts IS NULL\n",
    "      AND li_elem_counts IS NULL\n",
    "      AND video_elem_counts IS NULL\n",
    "      AND br_elem_counts IS NULL\n",
    "      AND tokens IS NULL\n",
    "      AND audience = \"everyone\"\n",
    "''')\n",
    "\n",
    "# Fetch all the rows that match the criteria\n",
    "rows = cursor.fetchall()\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "for row in tqdm(rows):\n",
    "    try:\n",
    "        url = row[0]\n",
    "        results = get_post_metadata_from_url(url)\n",
    "        insert_results_into_db(db_file, url, results)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
